{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implémentation de FastText"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import de librairies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import itertools\n",
    "\n",
    "import fasttext\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from constants import TRAINING_DATA_PATH, TEXT_COLUMN, LABEL_COLUMN, ALPHABETS_FT\n",
    "\n",
    "ALPHABETS = ALPHABETS_FT\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fonctions utiles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_fasttext_data(data, output_file):\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        for i in range(len(data)):\n",
    "            text = data.iloc[i][TEXT_COLUMN]\n",
    "            lang = data.iloc[i][LABEL_COLUMN]\n",
    "            # FastText attend le format: __label__LANG texte\n",
    "            f.write(f\"__label__{lang} {text}\\n\")            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_alphabet(text):\n",
    "    \"\"\"\n",
    "    Détecte l'alphabet principal utilisé dans un texte en analysant la fréquence des caractères.\n",
    "    Renvoie \"Inconnu\" si aucun alphabet connu n'est dominant.\n",
    "    \"\"\"\n",
    "    text = ''.join(c for c in text if not c.isspace() and c not in '.,;:!?-()[]{}\\'\\\"')\n",
    "    \n",
    "    if not text:\n",
    "        return \"Inconnu\", 0.0\n",
    "\n",
    "    counts = {name: 0 for name in ALPHABETS}\n",
    "    total_chars = len(text)\n",
    "\n",
    "    # Comptage des caractères par alphabet\n",
    "    for char in text:\n",
    "        char_code = ord(char)\n",
    "        for name, ranges in ALPHABETS.items():\n",
    "            if any(start <= char_code <= end for start, end in ranges):\n",
    "                counts[name] += 1\n",
    "                break  # Un caractère ne peut appartenir qu'à un seul alphabet\n",
    "\n",
    "    # Trouver l'alphabet le plus représenté\n",
    "    best_match = max(counts.items(), key=lambda x: x[1])\n",
    "    alphabet_name, max_count = best_match\n",
    "    percentage = (max_count / total_chars) * 100\n",
    "\n",
    "    # Si l'alphabet dominant représente moins de 50% du texte, on retourne \"Inconnu\"\n",
    "    if percentage < 50:\n",
    "        return \"Inconnu\", 0.0\n",
    "\n",
    "    return alphabet_name, percentage\n",
    "\n",
    "# Test de detection d'alphabets avec input\n",
    "\n",
    "# input_text = input(\"Entrez un texte à analyser: \")\n",
    "# detected, percentage = detect_alphabet(input_text)\n",
    "# print(f\"Texte: {input_text[:20]}...\")\n",
    "# print(f\"Détecté: {detected} ({percentage:.1f}%)\")\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Données sans label :  0\n",
      "            count  Label\n",
      "alphabet                \n",
      "Latin       28205    303\n",
      "Cyrillique   3299     34\n",
      "Arabe        2470     25\n",
      "Inconnu      2233     54\n",
      "Devanagari   1191     12\n",
      "Chinois       499      6\n",
      "Hébreu        235      3\n",
      "Géorgien      199      2\n",
      "Grec          199      2\n",
      "Gujarati      100      1\n",
      "Thaï          100      1\n",
      "Coréen        100      1\n",
      "Katakana       12      1\n",
      "Hiragana       12      1\n",
      "Données sans alphabet 0\n",
      "--------------------------------------------------\n",
      "Traitement de l'alphabet: Cyrillique\n",
      "Nombre d'instances par langue:\n",
      "Label\n",
      "myv    100\n",
      "hbs    100\n",
      "kbd    100\n",
      "rue    100\n",
      "uzb    100\n",
      "bel    100\n",
      "abk    100\n",
      "chv    100\n",
      "kaa    100\n",
      "tyv    100\n",
      "bak    100\n",
      "sah    100\n",
      "tat    100\n",
      "alt    100\n",
      "krc    100\n",
      "kaz    100\n",
      "crh    100\n",
      "ukr    100\n",
      "mon    100\n",
      "udm    100\n",
      "kjh    100\n",
      "kom    100\n",
      "mhr    100\n",
      "oss    100\n",
      "kir    100\n",
      "srp    100\n",
      "mkd    100\n",
      "bul    100\n",
      "uzn    100\n",
      "tgk    100\n",
      "bew    100\n",
      "che    100\n",
      "tuk     98\n",
      "Name: count, dtype: int64\n",
      "Split initial train/test pour Cyrillique: 2638 instances train, 660 instances test\n",
      "Évaluation de 1 combinaisons d'hyperparamètres\n",
      "Combinaison 1/1: {'lr': 0.2, 'dim': 30, 'epoch': 110, 'minn': 3, 'maxn': 4, 'wordNgrams': 1, 'minCount': 1, 'neg': 4}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  34071\n",
      "Number of labels: 33\n",
      "Progress: 100.0% words/sec/thread:  754496 lr:  0.000000 avg.loss:  0.632085 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nouvelle meilleure combinaison trouvée!\n",
      "  Précision: 0.8424\n",
      "\n",
      "Résultats de la recherche d'hyperparamètres pour Cyrillique:\n",
      "Meilleure précision : 0.8424\n",
      "Meilleurs hyperparamètres trouvés: {'lr': 0.2, 'dim': 30, 'epoch': 110, 'minn': 3, 'maxn': 4, 'wordNgrams': 1, 'minCount': 1, 'neg': 4}\n",
      "Nombre d'instances dans le set de test: 660\n",
      "Nombre de prédictions correctes: 556\n",
      "Modèle final sauvegardé: models/model_Cyrillique.bin\n",
      "--------------------------------------------------\n",
      "Traitement de l'alphabet: Latin\n",
      "Nombre d'instances par langue:\n",
      "Label\n",
      "arg    100\n",
      "mgh    100\n",
      "luo    100\n",
      "cab    100\n",
      "nnb    100\n",
      "      ... \n",
      "miq      3\n",
      "crs      2\n",
      "tvl      2\n",
      "pau      2\n",
      "gil      2\n",
      "Name: count, Length: 299, dtype: int64\n",
      "Split initial train/test pour Latin: 22560 instances train, 5641 instances test\n",
      "Évaluation de 1 combinaisons d'hyperparamètres\n",
      "Combinaison 1/1: {'lr': 0.4, 'dim': 40, 'epoch': 40, 'minn': 3, 'maxn': 7, 'wordNgrams': 1, 'minCount': 1, 'neg': 10}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  210698\n",
      "Number of labels: 299\n",
      "Progress: 100.0% words/sec/thread:  248929 lr:  0.000000 avg.loss:  1.307206 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nouvelle meilleure combinaison trouvée!\n",
      "  Précision: 0.7653\n",
      "\n",
      "Résultats de la recherche d'hyperparamètres pour Latin:\n",
      "Meilleure précision : 0.7653\n",
      "Meilleurs hyperparamètres trouvés: {'lr': 0.4, 'dim': 40, 'epoch': 40, 'minn': 3, 'maxn': 7, 'wordNgrams': 1, 'minCount': 1, 'neg': 10}\n",
      "Nombre d'instances dans le set de test: 5641\n",
      "Nombre de prédictions correctes: 4317\n",
      "Modèle final sauvegardé: models/model_Latin.bin\n",
      "--------------------------------------------------\n",
      "Traitement de l'alphabet: Arabe\n",
      "Nombre d'instances par langue:\n",
      "Label\n",
      "pus    100\n",
      "kur    100\n",
      "ara    100\n",
      "mzn    100\n",
      "arz    100\n",
      "pes    100\n",
      "tgk    100\n",
      "uig    100\n",
      "apc    100\n",
      "urd    100\n",
      "azb    100\n",
      "arb    100\n",
      "fas    100\n",
      "prs    100\n",
      "snd    100\n",
      "glk    100\n",
      "pnb    100\n",
      "ckb    100\n",
      "som     99\n",
      "afb     98\n",
      "ary     97\n",
      "acm     97\n",
      "aze     97\n",
      "ajp     97\n",
      "hau     85\n",
      "Name: count, dtype: int64\n",
      "Split initial train/test pour Arabe: 1976 instances train, 494 instances test\n",
      "Évaluation de 1 combinaisons d'hyperparamètres\n",
      "Combinaison 1/1: {'lr': 0.55, 'dim': 100, 'epoch': 110, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 8}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  21580\n",
      "Number of labels: 25\n",
      "Progress: 100.0% words/sec/thread:  307579 lr:  0.000000 avg.loss:  0.266341 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nouvelle meilleure combinaison trouvée!\n",
      "  Précision: 0.5972\n",
      "\n",
      "Résultats de la recherche d'hyperparamètres pour Arabe:\n",
      "Meilleure précision : 0.5972\n",
      "Meilleurs hyperparamètres trouvés: {'lr': 0.55, 'dim': 100, 'epoch': 110, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 8}\n",
      "Nombre d'instances dans le set de test: 494\n",
      "Nombre de prédictions correctes: 295\n",
      "Modèle final sauvegardé: models/model_Arabe.bin\n",
      "--------------------------------------------------\n",
      "Traitement de l'alphabet: Inconnu\n",
      "Nombre d'instances par langue:\n",
      "Label\n",
      "amh     100\n",
      "mal     100\n",
      "asm     100\n",
      "mya     100\n",
      "sat     100\n",
      "sin     100\n",
      "bod     100\n",
      "div     100\n",
      "tam     100\n",
      "hye     100\n",
      "tir     100\n",
      "pan     100\n",
      "lao     100\n",
      "ori     100\n",
      "kan     100\n",
      "bpy     100\n",
      "ory     100\n",
      "iku     100\n",
      "dzo     100\n",
      "khm     100\n",
      "jpn      64\n",
      "tbz      31\n",
      "hyw      25\n",
      "hau      15\n",
      "ksw      13\n",
      "kat       9\n",
      "guj       8\n",
      "fon       6\n",
      "yue       6\n",
      "bqc       6\n",
      "nana      5\n",
      "wuu       4\n",
      "uig       4\n",
      "ajp       3\n",
      "ary       3\n",
      "aze       3\n",
      "mon       3\n",
      "acm       3\n",
      "tat       3\n",
      "zho       2\n",
      "hin       2\n",
      "tuk       2\n",
      "afb       2\n",
      "Name: count, dtype: int64\n",
      "Split initial train/test pour Inconnu: 1777 instances train, 445 instances test\n",
      "Évaluation de 1 combinaisons d'hyperparamètres\n",
      "Combinaison 1/1: {'lr': 0.5, 'dim': 16, 'epoch': 100, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 5}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Read 0M words\n",
      "Number of words:  28091\n",
      "Number of labels: 43\n",
      "Progress: 100.0% words/sec/thread:  809794 lr:  0.000000 avg.loss:  0.440972 ETA:   0h 0m 0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Nouvelle meilleure combinaison trouvée!\n",
      "  Précision: 0.9011\n",
      "\n",
      "Résultats de la recherche d'hyperparamètres pour Inconnu:\n",
      "Meilleure précision : 0.9011\n",
      "Meilleurs hyperparamètres trouvés: {'lr': 0.5, 'dim': 16, 'epoch': 100, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 5}\n",
      "Nombre d'instances dans le set de test: 445\n",
      "Nombre de prédictions correctes: 401\n",
      "Modèle final sauvegardé: models/model_Inconnu.bin\n",
      "\n",
      "==================================================\n",
      "RÉSULTATS GLOBAUX SUR TOUS LES ALPHABETS\n",
      "Précision globale: 0.7692\n",
      "Nombre total d'échantillons testés: 7240\n",
      "Nombre total de prédictions correctes: 5569\n",
      "\n",
      "Résultats détaillés par alphabet:\n",
      "Alphabet: Inconnu\n",
      "  Précision test : 0.9011\n",
      "  Précision train: 0.9848\n",
      "  Hyperparamètres: {'lr': 0.5, 'dim': 16, 'epoch': 100, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 5}\n",
      "  Nombre d'échantillons testés: 445\n",
      "  Nombre de prédictions correctes: 401\n",
      "--------------------------------------------------\n",
      "Alphabet: Cyrillique\n",
      "  Précision test : 0.8424\n",
      "  Précision train: 0.9996\n",
      "  Hyperparamètres: {'lr': 0.2, 'dim': 30, 'epoch': 110, 'minn': 3, 'maxn': 4, 'wordNgrams': 1, 'minCount': 1, 'neg': 4}\n",
      "  Nombre d'échantillons testés: 660\n",
      "  Nombre de prédictions correctes: 556\n",
      "--------------------------------------------------\n",
      "Alphabet: Latin\n",
      "  Précision test : 0.7653\n",
      "  Précision train: 0.9878\n",
      "  Hyperparamètres: {'lr': 0.4, 'dim': 40, 'epoch': 40, 'minn': 3, 'maxn': 7, 'wordNgrams': 1, 'minCount': 1, 'neg': 10}\n",
      "  Nombre d'échantillons testés: 5641\n",
      "  Nombre de prédictions correctes: 4317\n",
      "--------------------------------------------------\n",
      "Alphabet: Arabe\n",
      "  Précision test : 0.5972\n",
      "  Précision train: 1.0000\n",
      "  Hyperparamètres: {'lr': 0.55, 'dim': 100, 'epoch': 110, 'minn': 3, 'maxn': 5, 'wordNgrams': 1, 'minCount': 1, 'neg': 8}\n",
      "  Nombre d'échantillons testés: 494\n",
      "  Nombre de prédictions correctes: 295\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Extraction des données\n",
    "data = pd.read_csv(TRAINING_DATA_PATH)\n",
    "\n",
    "# Prise en compte du label nan (qui est le code d'une langue)\n",
    "data = data.fillna(\"nana\")\n",
    "\n",
    "# Nettoyage des textes\n",
    "def clean_text(text):\n",
    "    text = text.lower()  # Convertir en minuscule\n",
    "    text = re.sub(r'[\\x00-\\x08\\x0b\\x0c\\x0e-\\x1f\\x7f-\\x9f]', '', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Supprimer espaces multiples\n",
    "    return text\n",
    "\n",
    "data[TEXT_COLUMN] = data[TEXT_COLUMN].apply(clean_text)\n",
    "\n",
    "# Nettoyage des données\n",
    "print(\"Données sans label : \", data[LABEL_COLUMN].isna().sum())\n",
    "data = data.dropna(subset=[LABEL_COLUMN])\n",
    "\n",
    "# Ajouter une colonne pour l'alphabet détecté\n",
    "data['alphabet'], data['alphabet_percentage'] = zip(*data[TEXT_COLUMN].map(detect_alphabet))\n",
    "\n",
    "# Répartition des alphabets détectés, donné le nombre de données et de labels pour chaque alphabet\n",
    "alphabet_counts = data['alphabet'].value_counts()\n",
    "# Ajouter le nombre de labels par alphabet dans le meme dataframe alphabet counts\n",
    "alphabet_counts = pd.concat([alphabet_counts, data.groupby('alphabet')[LABEL_COLUMN].nunique()], axis=1)\n",
    "print(alphabet_counts)\n",
    "\n",
    "# Vérification que toutes les données ont un alphabet détecté\n",
    "print(\"Données sans alphabet\", data['alphabet'].isna().sum())\n",
    "\n",
    "# Grille d'hyperparamètres à tester\n",
    "\n",
    "# Base param grid\n",
    "# param_grid = {\n",
    "#     \"lr\": [0.4, 0.5, 0.6],  # Autour de 0.5\n",
    "#     \"dim\": [16, 20],  # Autour de 16\n",
    "#     \"epoch\": [50, 60],  # Autour de 50\n",
    "#     \"minn\": [2, 3],  # Autour de 2\n",
    "#     \"maxn\": [5, 6],  # Autour de 6\n",
    "#     \"wordNgrams\": [1],  # Pas de modification\n",
    "#     \"minCount\": [1],  # Pas de modification\n",
    "#     \"neg\": [5, 6],  # Autour de 5\n",
    "# }\n",
    "\n",
    "param_grid_global = {\n",
    "    \"Inconnu\": {\n",
    "        \"lr\": [0.5],\n",
    "        \"dim\": [16],\n",
    "        \"epoch\": [100],\n",
    "        \"minn\": [3],\n",
    "        \"maxn\": [5],\n",
    "        \"wordNgrams\": [1],\n",
    "        \"minCount\": [1],\n",
    "        \"neg\": [5],\n",
    "    },\n",
    "    \"Cyrillique\": {\n",
    "        \"lr\": [0.2],\n",
    "        \"dim\": [30],\n",
    "        \"epoch\": [110],\n",
    "        \"minn\": [3],\n",
    "        \"maxn\": [4],\n",
    "        \"wordNgrams\": [1],\n",
    "        \"minCount\": [1],\n",
    "        \"neg\": [4],\n",
    "    },\n",
    "    \"Latin\": {\n",
    "        \"lr\": [0.4],\n",
    "        \"dim\": [40],\n",
    "        \"epoch\": [40],\n",
    "        \"minn\": [3],\n",
    "        \"maxn\": [7],\n",
    "        \"wordNgrams\": [1],\n",
    "        \"minCount\": [1],\n",
    "        \"neg\": [10],\n",
    "    },\n",
    "    \"Arabe\": {\n",
    "        \"lr\": [0.55],\n",
    "        \"dim\": [100],\n",
    "        \"epoch\": [110],\n",
    "        \"minn\": [3],\n",
    "        \"maxn\": [5],\n",
    "        \"wordNgrams\": [1],\n",
    "        \"minCount\": [1],\n",
    "        \"neg\": [8],\n",
    "    }\n",
    "}\n",
    "\n",
    "# Créer le dossier pour les données temporaires si nécessaire\n",
    "os.makedirs(f\"data/alphabet\", exist_ok=True)\n",
    "os.makedirs(f\"models\", exist_ok=True)\n",
    "\n",
    "# Variables globales pour les métriques finales\n",
    "global_instances = 0\n",
    "global_right_predictions = 0\n",
    "global_results = []\n",
    "\n",
    "# Boucle sur chaque alphabet (choisir les alphabets que l'on veut traiter)\n",
    "for alphabet in [\"Cyrillique\", \"Latin\", \"Arabe\", \"Inconnu\"]:\n",
    "    print(\"-\" * 50)\n",
    "    print(f\"Traitement de l'alphabet: {alphabet}\")\n",
    "    \n",
    "    # Filtrer les données pour l'alphabet actuel\n",
    "    alphabet_data = data[data['alphabet'] == alphabet]\n",
    "\n",
    "    # Enlever les instances qui sont les seuls représentants de leur langue\n",
    "    label_counts = alphabet_data[LABEL_COLUMN].value_counts()\n",
    "    single_instances = label_counts[label_counts == 1].index\n",
    "    alphabet_data = alphabet_data[~alphabet_data[LABEL_COLUMN].isin(single_instances)]\n",
    "    \n",
    "    \n",
    "    # Afficher le nombre d'instance pour chaque label\n",
    "    print(\"Nombre d'instances par langue:\")\n",
    "    print(alphabet_data[LABEL_COLUMN].value_counts())\n",
    "    \n",
    "    # 1. SPLIT INITIAL TRAIN/TEST\n",
    "    train_data, test_data = train_test_split(\n",
    "        alphabet_data, \n",
    "        test_size=0.20, \n",
    "        shuffle=True, \n",
    "        random_state=10, \n",
    "        stratify=alphabet_data[LABEL_COLUMN]\n",
    "    )\n",
    "\n",
    "    print(f\"Split initial train/test pour {alphabet}: {len(train_data)} instances train, {len(test_data)} instances test\")\n",
    "    \n",
    "    \n",
    "    # Variables pour stocker les résultats de la validation croisée\n",
    "    best_params = None\n",
    "    best_model = None\n",
    "    best_test_accuracy = 0\n",
    "    \n",
    "    # Générer toutes les combinaisons d'hyperparamètres\n",
    "    all_param_combinations = [dict(zip(param_grid_global[alphabet].keys(), params)) \n",
    "                             for params in itertools.product(*param_grid_global[alphabet].values())]\n",
    "    \n",
    "    print(f\"Évaluation de {len(all_param_combinations)} combinaisons d'hyperparamètres\")\n",
    "    \n",
    "    # Prépare les données pour FastText\n",
    "    train_file = f\"data/alphabet/train_{alphabet}.txt\"\n",
    "    test_file = f\"data/alphabet/test_{alphabet}.txt\"\n",
    "\n",
    "    prepare_fasttext_data(data = train_data, output_file=train_file)\n",
    "    prepare_fasttext_data(data = test_data, output_file=test_file)\n",
    "    \n",
    "    # Itérer d'abord sur les combinaisons d'hyperparamètres\n",
    "    for param_idx, param_combination in enumerate(all_param_combinations):\n",
    "        print(f\"Combinaison {param_idx+1}/{len(all_param_combinations)}: {param_combination}\")\n",
    "    \n",
    "\n",
    "        model = fasttext.train_supervised(\n",
    "            input=train_file,\n",
    "            lr=param_combination['lr'],\n",
    "            dim=param_combination['dim'],\n",
    "            epoch=param_combination['epoch'],\n",
    "            wordNgrams=param_combination['wordNgrams'],\n",
    "            minCount=param_combination['minCount'],\n",
    "            minn=param_combination['minn'],\n",
    "            maxn=param_combination['maxn'],\n",
    "            neg=param_combination['neg'],\n",
    "        )\n",
    "        \n",
    "        # Évaluer le modèle sur les données de validation\n",
    "        test_result = model.test(test_file)\n",
    "        test_accuracy = test_result[1]\n",
    "        \n",
    "        train_result = model.test(train_file)\n",
    "        train_accuracy = train_result[1]\n",
    "        \n",
    "        # Conserver les meilleurs hyperparamètres\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            best_train_accuracy = train_accuracy\n",
    "            best_params = param_combination\n",
    "            best_model = model\n",
    "            print(f\"  Nouvelle meilleure combinaison trouvée!\")\n",
    "            print(f\"  Précision: {test_accuracy:.4f}\")\n",
    "            \n",
    "\n",
    "    # Afficher les meilleurs hyperparamètres trouvés\n",
    "    print(f\"\\nRésultats de la recherche d'hyperparamètres pour {alphabet}:\")\n",
    "    print(f\"Meilleure précision : {best_test_accuracy:.4f}\")\n",
    "    print(f\"Meilleurs hyperparamètres trouvés: {best_params}\")\n",
    "    \n",
    "    # Mettre à jour les compteurs globaux\n",
    "    nb_test_samples = len(test_data)\n",
    "    nb_right_predictions = int(best_test_accuracy * nb_test_samples)\n",
    "    print(f\"Nombre d'instances dans le set de test: {nb_test_samples}\")\n",
    "    print(f\"Nombre de prédictions correctes: {nb_right_predictions}\")\n",
    "    \n",
    "    # Afficher une matrice de confusion du set de test\n",
    "    # Obtenir la liste des labels uniques et créer un mapping label → index\n",
    "    unique_labels = sorted(test_data[LABEL_COLUMN].unique())  \n",
    "    label_to_index = {label: idx for idx, label in enumerate(unique_labels)}\n",
    "    \n",
    "    global_instances += nb_test_samples\n",
    "    global_right_predictions += nb_right_predictions\n",
    "    \n",
    "    # Enregistrer les résultats pour cet alphabet\n",
    "    global_results.append({\n",
    "        'alphabet': alphabet,\n",
    "        'test_accuracy': best_test_accuracy,\n",
    "        'train_accuracy': best_train_accuracy,\n",
    "        'hyperparameters': best_params,\n",
    "        'test_samples': nb_test_samples,\n",
    "        'correct_predictions': nb_right_predictions\n",
    "    })\n",
    "    \n",
    "    # Sauvegarder le modèle final\n",
    "    best_model.save_model(f\"models/model_{alphabet}.bin\")\n",
    "    print(f\"Modèle final sauvegardé: models/model_{alphabet}.bin\")\n",
    "\n",
    "# Calculer la précision globale sur tous les alphabets\n",
    "global_accuracy = global_right_predictions / global_instances if global_instances > 0 else 0\n",
    "\n",
    "# Afficher les résultats globaux\n",
    "print(\"\\n\" + \"=\" * 50)\n",
    "print(\"RÉSULTATS GLOBAUX SUR TOUS LES ALPHABETS\")\n",
    "print(f\"Précision globale: {global_accuracy:.4f}\")\n",
    "print(f\"Nombre total d'échantillons testés: {global_instances}\")\n",
    "print(f\"Nombre total de prédictions correctes: {global_right_predictions}\")\n",
    "\n",
    "# Afficher les résultats détaillés par alphabet\n",
    "print(\"\\nRésultats détaillés par alphabet:\")\n",
    "for result in sorted(global_results, key=lambda x: x['test_accuracy'], reverse=True):\n",
    "    print(f\"Alphabet: {result['alphabet']}\")\n",
    "    print(f\"  Précision test : {result['test_accuracy']:.4f}\")\n",
    "    print(f\"  Précision train: {result['train_accuracy']:.4f}\")\n",
    "    print(f\"  Hyperparamètres: {result['hyperparameters']}\")\n",
    "    print(f\"  Nombre d'échantillons testés: {result['test_samples']}\")\n",
    "    print(f\"  Nombre de prédictions correctes: {result['correct_predictions']}\")\n",
    "    print(\"-\" * 50)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "fasttext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
